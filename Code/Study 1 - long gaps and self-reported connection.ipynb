{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "451200d0",
   "metadata": {},
   "source": [
    "How do participantâ€™s reports of connection changed when long gaps naturally occurred in their conversations? \n",
    "\n",
    "We compute average connection ratings for every instance of a long gap across all the conversations. As a comparison, we compute average connection ratings computed in 2-second increments for the 6 seconds immediately preceding the long gap and the 6 seconds immediately following the long gap. \n",
    "\n",
    "To make comparisons across conversations, we compute connection difference scores for every time-point by subtracting the connection ratings during the long gap from the connection ratings at each time-point. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd9bd2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import os.path\n",
    "import glob\n",
    "from scipy import stats\n",
    "import statsmodels\n",
    "\n",
    "base_dir = os.path.dirname(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d905d50f",
   "metadata": {},
   "source": [
    "## Strangers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "71d71c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "long_gap_connection = pd.DataFrame()\n",
    "long_gap_connection = long_gap_connection.fillna(0)\n",
    "counter = 0\n",
    "\n",
    "flist = glob.glob(os.path.join(base_dir, 'Analyses', 'turn_taking', 'strangers', '*.csv'))\n",
    "\n",
    "for file in flist:\n",
    "    \n",
    "    convo = file.split('/')[-1].split('.csv')[0]\n",
    "    id_1 = file.split('/')[-1].split('_')[0]\n",
    "    id_2 = file.split('_')[-1].split('.csv')[0]\n",
    "    \n",
    "    data = pd.read_csv(file)\n",
    "    \n",
    "    long_gaps = data.loc[data['gap_length'] > 2000]\n",
    "    \n",
    "    if len(long_gaps) > 0:\n",
    "        \n",
    "        connection_1 = pd.read_csv(os.path.join(base_dir, 'Data', 'continuous_connection_ratings', 'strangers', '{}_{}.csv'.format(id_1, id_2)))\n",
    "        connection_2 = pd.read_csv(os.path.join(base_dir, 'Data', 'continuous_connection_ratings', 'strangers', '{}_{}.csv'.format(id_2, id_1)))\n",
    "        \n",
    "        connection_1['time_msec'] = connection_1['adjustedTime'] * 1000\n",
    "        connection_2['time_msec'] = connection_2['adjustedTime'] * 1000\n",
    "        \n",
    "        for i in list(long_gaps.index):\n",
    "            \n",
    "            if (i > 0):\n",
    "                \n",
    "                if (data.at[i-1, 'turn_end_msec'] > 6000) & (data.at[i, 'turn_start_msec'] < 594000): # might change and just give NaNs for ones that are too close\n",
    "\n",
    "                    # the long gap\n",
    "                    start_of_gap = data.at[i-1, 'turn_end_msec']\n",
    "                    end_of_gap = data.at[i, 'turn_start_msec']\n",
    "                    \n",
    "                    connection_1_long_gap_subset = connection_1.loc[(connection_1['time_msec'] > start_of_gap) & (connection_1['time_msec'] < end_of_gap)].reset_index(drop=True)\n",
    "                    connection_1_during_long_gap = np.mean(connection_1_long_gap_subset['Rating'])\n",
    "                    \n",
    "                    connection_2_long_gap_subset = connection_2.loc[(connection_2['time_msec'] > start_of_gap) & (connection_2['time_msec'] < end_of_gap)].reset_index(drop=True)\n",
    "                    connection_2_during_long_gap = np.mean(connection_2_long_gap_subset['Rating'])\n",
    "\n",
    "                    # 1 chunk before long gap\n",
    "\n",
    "                    start_connection_before_1 = start_of_gap - 2000\n",
    "                    end_connection_before_1 = start_of_gap\n",
    "                    \n",
    "                    connection_1_before_1_subset = connection_1.loc[(connection_1['time_msec'] > start_connection_before_1) & (connection_1['time_msec'] < end_connection_before_1)].reset_index(drop=True)\n",
    "                    connection_1_before_long_gap_1 = np.mean(connection_1_before_1_subset['Rating'])\n",
    "                    \n",
    "                    connection_2_before_1_subset = connection_2.loc[(connection_2['time_msec'] > start_connection_before_1) & (connection_2['time_msec'] < end_connection_before_1)].reset_index(drop=True)\n",
    "                    connection_2_before_long_gap_1 = np.mean(connection_2_before_1_subset['Rating'])\n",
    "\n",
    "                    # 2 chunks before long gap\n",
    "\n",
    "                    start_connection_before_2 = start_of_gap - 4000 \n",
    "                    end_connection_before_2 = start_of_gap - 2000\n",
    "                    \n",
    "                    connection_1_before_2_subset = connection_1.loc[(connection_1['time_msec'] > start_connection_before_2) & (connection_1['time_msec'] < end_connection_before_2)].reset_index(drop=True)\n",
    "                    connection_1_before_long_gap_2 = np.mean(connection_1_before_2_subset['Rating'])\n",
    "                    \n",
    "                    connection_2_before_2_subset = connection_2.loc[(connection_2['time_msec'] > start_connection_before_2) & (connection_2['time_msec'] < end_connection_before_2)].reset_index(drop=True)\n",
    "                    connection_2_before_long_gap_2 = np.mean(connection_2_before_2_subset['Rating'])\n",
    "\n",
    "                    # 3 chunks before long gap\n",
    "\n",
    "                    start_connection_before_3 = start_of_gap - 6000\n",
    "                    end_connection_before_3 = start_of_gap - 4000\n",
    "\n",
    "                    connection_1_before_3_subset = connection_1.loc[(connection_1['time_msec'] > start_connection_before_3) & (connection_1['time_msec'] < end_connection_before_3)].reset_index(drop=True)\n",
    "                    connection_1_before_long_gap_3 = np.mean(connection_1_before_3_subset['Rating'])\n",
    "                    \n",
    "                    connection_2_before_3_subset = connection_2.loc[(connection_2['time_msec'] > start_connection_before_3) & (connection_2['time_msec'] < end_connection_before_3)].reset_index(drop=True)\n",
    "                    connection_2_before_long_gap_3 = np.mean(connection_2_before_3_subset['Rating'])\n",
    "                    \n",
    "\n",
    "                    # 1 chunk after long gap \n",
    "\n",
    "                    start_connection_after_1 = end_of_gap\n",
    "                    end_connection_after_1 = end_of_gap + 2000\n",
    "                    \n",
    "                    connection_1_after_1_subset = connection_1.loc[(connection_1['time_msec'] > start_connection_after_1) & (connection_1['time_msec'] < end_connection_after_1)].reset_index(drop=True)\n",
    "                    connection_1_after_long_gap_1 = np.mean(connection_1_after_1_subset['Rating'])\n",
    "                    \n",
    "                    connection_2_after_1_subset = connection_2.loc[(connection_2['time_msec'] > start_connection_after_1) & (connection_2['time_msec'] < end_connection_after_1)].reset_index(drop=True)\n",
    "                    connection_2_after_long_gap_1 = np.mean(connection_2_after_1_subset['Rating'])\n",
    "\n",
    "                    # 2 chunks after long gap \n",
    "\n",
    "                    start_connection_after_2 = end_of_gap + 2000 \n",
    "                    end_connection_after_2 = end_of_gap + 4000\n",
    "                    \n",
    "                    connection_1_after_2_subset = connection_1.loc[(connection_1['time_msec'] > start_connection_after_2) & (connection_1['time_msec'] < end_connection_after_2)].reset_index(drop=True)\n",
    "                    connection_1_after_long_gap_2 = np.mean(connection_1_after_2_subset['Rating'])\n",
    "                    \n",
    "                    connection_2_after_2_subset = connection_2.loc[(connection_2['time_msec'] > start_connection_after_2) & (connection_2['time_msec'] < end_connection_after_2)].reset_index(drop=True)\n",
    "                    connection_2_after_long_gap_2 = np.mean(connection_2_after_2_subset['Rating'])\n",
    "\n",
    "                    # 3 chunks after long gap \n",
    "\n",
    "                    start_connection_after_3 = end_of_gap + 4000\n",
    "                    end_connection_after_3 = end_of_gap + 6000\n",
    "                    \n",
    "                    connection_1_after_3_subset = connection_1.loc[(connection_1['time_msec'] > start_connection_after_3) & (connection_1['time_msec'] < end_connection_after_3)].reset_index(drop=True)\n",
    "                    connection_1_after_long_gap_3 = np.mean(connection_1_after_3_subset['Rating'])\n",
    "                    \n",
    "                    connection_2_after_3_subset = connection_2.loc[(connection_2['time_msec'] > start_connection_after_3) & (connection_2['time_msec'] < end_connection_after_3)].reset_index(drop=True)\n",
    "                    connection_2_after_long_gap_3 = np.mean(connection_2_after_3_subset['Rating'])\n",
    "\n",
    "\n",
    "                    long_gap_connection.at[counter, 'subID'] = id_1\n",
    "                    long_gap_connection.at[counter, 'partnerID'] = id_2\n",
    "                    long_gap_connection.at[counter, 'dyad'] = convo\n",
    "                    long_gap_connection.at[counter, 'connection_3_before'] = connection_1_before_long_gap_3\n",
    "                    long_gap_connection.at[counter, 'connection_2_before'] = connection_1_before_long_gap_2\n",
    "                    long_gap_connection.at[counter, 'connection_1_before'] = connection_1_before_long_gap_1\n",
    "                    long_gap_connection.at[counter, 'connection_long_gap'] = connection_1_during_long_gap\n",
    "                    long_gap_connection.at[counter, 'connection_1_after'] = connection_1_after_long_gap_1\n",
    "                    long_gap_connection.at[counter, 'connection_2_after'] = connection_1_after_long_gap_2\n",
    "                    long_gap_connection.at[counter, 'connection_3_after'] = connection_1_after_long_gap_3\n",
    "                    long_gap_connection.at[counter, 'gap_length'] = data.at[i, 'gap_length']\n",
    "\n",
    "                    counter += 1\n",
    "\n",
    "                    long_gap_connection.at[counter, 'subID'] = id_2\n",
    "                    long_gap_connection.at[counter, 'partnerID'] = id_1\n",
    "                    long_gap_connection.at[counter, 'dyad'] = convo\n",
    "                    long_gap_connection.at[counter, 'connection_3_before'] = connection_2_before_long_gap_3\n",
    "                    long_gap_connection.at[counter, 'connection_2_before'] = connection_2_before_long_gap_2\n",
    "                    long_gap_connection.at[counter, 'connection_1_before'] = connection_2_before_long_gap_1\n",
    "                    long_gap_connection.at[counter, 'connection_long_gap'] = connection_2_during_long_gap\n",
    "                    long_gap_connection.at[counter, 'connection_1_after'] = connection_2_after_long_gap_1\n",
    "                    long_gap_connection.at[counter, 'connection_2_after'] = connection_2_after_long_gap_2\n",
    "                    long_gap_connection.at[counter, 'connection_3_after'] = connection_2_after_long_gap_3\n",
    "                    long_gap_connection.at[counter, 'gap_length'] = data.at[i, 'gap_length']\n",
    "\n",
    "                    counter += 1\n",
    "        \n",
    "long_gap_connection['change_in_connection_3_before'] = long_gap_connection['connection_3_before'] - long_gap_connection['connection_long_gap']\n",
    "long_gap_connection['change_in_connection_2_before'] = long_gap_connection['connection_2_before'] - long_gap_connection['connection_long_gap']\n",
    "long_gap_connection['change_in_connection_1_before'] = long_gap_connection['connection_1_before'] - long_gap_connection['connection_long_gap']\n",
    "long_gap_connection['change_in_connection_1_after'] = long_gap_connection['connection_1_after'] - long_gap_connection['connection_long_gap']\n",
    "long_gap_connection['change_in_connection_2_after'] = long_gap_connection['connection_2_after'] - long_gap_connection['connection_long_gap']\n",
    "long_gap_connection['change_in_connection_3_after'] = long_gap_connection['connection_3_after'] - long_gap_connection['connection_long_gap']\n",
    "\n",
    "long_gap_connection.to_csv(os.path.join(base_dir, 'Analyses',\n",
    "                              'long_gap_connection_strangers.csv'),\n",
    "                        encoding='utf-8', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba67d704",
   "metadata": {},
   "source": [
    "## Friends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff7819ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "long_gap_connection_friends = pd.DataFrame()\n",
    "long_gap_connection_friends = long_gap_connection_friends.fillna(0)\n",
    "counter = 0\n",
    "\n",
    "flist = glob.glob(os.path.join(base_dir, 'Analyses', 'turn_taking', 'friends', '*.csv'))\n",
    "\n",
    "for file in flist:\n",
    "    \n",
    "    convo = file.split('/')[-1].split('.csv')[0]\n",
    "    id_1 = file.split('/')[-1].split('_')[0]\n",
    "    id_2 = file.split('_')[-1].split('.csv')[0]\n",
    "    \n",
    "    data = pd.read_csv(file)\n",
    "    \n",
    "    long_gaps = data.loc[data['gap_length'] > 2000]\n",
    "    \n",
    "    if len(long_gaps) > 0:\n",
    "        \n",
    "        connection_1 = pd.read_csv(os.path.join(base_dir, 'Data', 'continuous_connection_ratings', 'friends', '{}_{}.csv'.format(id_1, id_2)))\n",
    "        connection_2 = pd.read_csv(os.path.join(base_dir, 'Data', 'continuous_connection_ratings', 'friends', '{}_{}.csv'.format(id_2, id_1)))\n",
    "        \n",
    "        connection_1['time_msec'] = connection_1['adjustedTime'] * 1000\n",
    "        connection_2['time_msec'] = connection_2['adjustedTime'] * 1000\n",
    "        \n",
    "        for i in list(long_gaps.index):\n",
    "            \n",
    "            if (i > 0):\n",
    "                \n",
    "                if (data.at[i-1, 'turn_end_msec'] > 6000) & (data.at[i, 'turn_start_msec'] < 594000): # might change and just give NaNs for ones that are too close\n",
    "\n",
    "                    # the long gap\n",
    "                    start_of_gap = data.at[i-1, 'turn_end_msec']\n",
    "                    end_of_gap = data.at[i, 'turn_start_msec']\n",
    "                    \n",
    "                    connection_1_long_gap_subset = connection_1.loc[(connection_1['time_msec'] > start_of_gap) & (connection_1['time_msec'] < end_of_gap)].reset_index(drop=True)\n",
    "                    connection_1_during_long_gap = np.mean(connection_1_long_gap_subset['Rating'])\n",
    "                    \n",
    "                    connection_2_long_gap_subset = connection_2.loc[(connection_2['time_msec'] > start_of_gap) & (connection_2['time_msec'] < end_of_gap)].reset_index(drop=True)\n",
    "                    connection_2_during_long_gap = np.mean(connection_2_long_gap_subset['Rating'])\n",
    "\n",
    "                    # 1 chunk before long gap\n",
    "\n",
    "                    start_connection_before_1 = start_of_gap - 2000\n",
    "                    end_connection_before_1 = start_of_gap\n",
    "                    \n",
    "                    connection_1_before_1_subset = connection_1.loc[(connection_1['time_msec'] > start_connection_before_1) & (connection_1['time_msec'] < end_connection_before_1)].reset_index(drop=True)\n",
    "                    connection_1_before_long_gap_1 = np.mean(connection_1_before_1_subset['Rating'])\n",
    "                    \n",
    "                    connection_2_before_1_subset = connection_2.loc[(connection_2['time_msec'] > start_connection_before_1) & (connection_2['time_msec'] < end_connection_before_1)].reset_index(drop=True)\n",
    "                    connection_2_before_long_gap_1 = np.mean(connection_2_before_1_subset['Rating'])\n",
    "\n",
    "                    # 2 chunks before long gap\n",
    "\n",
    "                    start_connection_before_2 = start_of_gap - 4000 \n",
    "                    end_connection_before_2 = start_of_gap - 2000\n",
    "                    \n",
    "                    connection_1_before_2_subset = connection_1.loc[(connection_1['time_msec'] > start_connection_before_2) & (connection_1['time_msec'] < end_connection_before_2)].reset_index(drop=True)\n",
    "                    connection_1_before_long_gap_2 = np.mean(connection_1_before_2_subset['Rating'])\n",
    "                    \n",
    "                    connection_2_before_2_subset = connection_2.loc[(connection_2['time_msec'] > start_connection_before_2) & (connection_2['time_msec'] < end_connection_before_2)].reset_index(drop=True)\n",
    "                    connection_2_before_long_gap_2 = np.mean(connection_2_before_2_subset['Rating'])\n",
    "\n",
    "                    # 3 chunks before long gap\n",
    "\n",
    "                    start_connection_before_3 = start_of_gap - 6000\n",
    "                    end_connection_before_3 = start_of_gap - 4000\n",
    "\n",
    "                    connection_1_before_3_subset = connection_1.loc[(connection_1['time_msec'] > start_connection_before_3) & (connection_1['time_msec'] < end_connection_before_3)].reset_index(drop=True)\n",
    "                    connection_1_before_long_gap_3 = np.mean(connection_1_before_3_subset['Rating'])\n",
    "                    \n",
    "                    connection_2_before_3_subset = connection_2.loc[(connection_2['time_msec'] > start_connection_before_3) & (connection_2['time_msec'] < end_connection_before_3)].reset_index(drop=True)\n",
    "                    connection_2_before_long_gap_3 = np.mean(connection_2_before_3_subset['Rating'])\n",
    "                    \n",
    "\n",
    "                    # 1 chunk after long gap \n",
    "\n",
    "                    start_connection_after_1 = end_of_gap\n",
    "                    end_connection_after_1 = end_of_gap + 2000\n",
    "                    \n",
    "                    connection_1_after_1_subset = connection_1.loc[(connection_1['time_msec'] > start_connection_after_1) & (connection_1['time_msec'] < end_connection_after_1)].reset_index(drop=True)\n",
    "                    connection_1_after_long_gap_1 = np.mean(connection_1_after_1_subset['Rating'])\n",
    "                    \n",
    "                    connection_2_after_1_subset = connection_2.loc[(connection_2['time_msec'] > start_connection_after_1) & (connection_2['time_msec'] < end_connection_after_1)].reset_index(drop=True)\n",
    "                    connection_2_after_long_gap_1 = np.mean(connection_2_after_1_subset['Rating'])\n",
    "\n",
    "                    # 2 chunks after long gap \n",
    "\n",
    "                    start_connection_after_2 = end_of_gap + 2000 \n",
    "                    end_connection_after_2 = end_of_gap + 4000\n",
    "                    \n",
    "                    connection_1_after_2_subset = connection_1.loc[(connection_1['time_msec'] > start_connection_after_2) & (connection_1['time_msec'] < end_connection_after_2)].reset_index(drop=True)\n",
    "                    connection_1_after_long_gap_2 = np.mean(connection_1_after_2_subset['Rating'])\n",
    "                    \n",
    "                    connection_2_after_2_subset = connection_2.loc[(connection_2['time_msec'] > start_connection_after_2) & (connection_2['time_msec'] < end_connection_after_2)].reset_index(drop=True)\n",
    "                    connection_2_after_long_gap_2 = np.mean(connection_2_after_2_subset['Rating'])\n",
    "\n",
    "                    # 3 chunks after long gap \n",
    "\n",
    "                    start_connection_after_3 = end_of_gap + 4000\n",
    "                    end_connection_after_3 = end_of_gap + 6000\n",
    "                    \n",
    "                    connection_1_after_3_subset = connection_1.loc[(connection_1['time_msec'] > start_connection_after_3) & (connection_1['time_msec'] < end_connection_after_3)].reset_index(drop=True)\n",
    "                    connection_1_after_long_gap_3 = np.mean(connection_1_after_3_subset['Rating'])\n",
    "                    \n",
    "                    connection_2_after_3_subset = connection_2.loc[(connection_2['time_msec'] > start_connection_after_3) & (connection_2['time_msec'] < end_connection_after_3)].reset_index(drop=True)\n",
    "                    connection_2_after_long_gap_3 = np.mean(connection_2_after_3_subset['Rating'])\n",
    "\n",
    "\n",
    "                    long_gap_connection_friends.at[counter, 'subID'] = id_1\n",
    "                    long_gap_connection_friends.at[counter, 'partnerID'] = id_2\n",
    "                    long_gap_connection_friends.at[counter, 'dyad'] = convo\n",
    "                    long_gap_connection_friends.at[counter, 'connection_3_before'] = connection_1_before_long_gap_3\n",
    "                    long_gap_connection_friends.at[counter, 'connection_2_before'] = connection_1_before_long_gap_2\n",
    "                    long_gap_connection_friends.at[counter, 'connection_1_before'] = connection_1_before_long_gap_1\n",
    "                    long_gap_connection_friends.at[counter, 'connection_long_gap'] = connection_1_during_long_gap\n",
    "                    long_gap_connection_friends.at[counter, 'connection_1_after'] = connection_1_after_long_gap_1\n",
    "                    long_gap_connection_friends.at[counter, 'connection_2_after'] = connection_1_after_long_gap_2\n",
    "                    long_gap_connection_friends.at[counter, 'connection_3_after'] = connection_1_after_long_gap_3\n",
    "                    long_gap_connection_friends.at[counter, 'gap_length'] = data.at[i, 'gap_length']\n",
    "\n",
    "                    counter += 1\n",
    "\n",
    "                    long_gap_connection_friends.at[counter, 'subID'] = id_2\n",
    "                    long_gap_connection_friends.at[counter, 'partnerID'] = id_1\n",
    "                    long_gap_connection_friends.at[counter, 'dyad'] = convo\n",
    "                    long_gap_connection_friends.at[counter, 'connection_3_before'] = connection_2_before_long_gap_3\n",
    "                    long_gap_connection_friends.at[counter, 'connection_2_before'] = connection_2_before_long_gap_2\n",
    "                    long_gap_connection_friends.at[counter, 'connection_1_before'] = connection_2_before_long_gap_1\n",
    "                    long_gap_connection_friends.at[counter, 'connection_long_gap'] = connection_2_during_long_gap\n",
    "                    long_gap_connection_friends.at[counter, 'connection_1_after'] = connection_2_after_long_gap_1\n",
    "                    long_gap_connection_friends.at[counter, 'connection_2_after'] = connection_2_after_long_gap_2\n",
    "                    long_gap_connection_friends.at[counter, 'connection_3_after'] = connection_2_after_long_gap_3\n",
    "                    long_gap_connection_friends.at[counter, 'gap_length'] = data.at[i, 'gap_length']\n",
    "\n",
    "                    counter += 1\n",
    "        \n",
    "long_gap_connection_friends['change_in_connection_3_before'] = long_gap_connection_friends['connection_3_before'] - long_gap_connection_friends['connection_long_gap']\n",
    "long_gap_connection_friends['change_in_connection_2_before'] = long_gap_connection_friends['connection_2_before'] - long_gap_connection_friends['connection_long_gap']\n",
    "long_gap_connection_friends['change_in_connection_1_before'] = long_gap_connection_friends['connection_1_before'] - long_gap_connection_friends['connection_long_gap']\n",
    "long_gap_connection_friends['change_in_connection_1_after'] = long_gap_connection_friends['connection_1_after'] - long_gap_connection_friends['connection_long_gap']\n",
    "long_gap_connection_friends['change_in_connection_2_after'] = long_gap_connection_friends['connection_2_after'] - long_gap_connection_friends['connection_long_gap']\n",
    "long_gap_connection_friends['change_in_connection_3_after'] = long_gap_connection_friends['connection_3_after'] - long_gap_connection_friends['connection_long_gap']\n",
    "\n",
    "long_gap_connection_friends.to_csv(os.path.join(base_dir, 'Analyses',\n",
    "                              'long_gap_connection_friends.csv'),\n",
    "                        encoding='utf-8', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bab96b1",
   "metadata": {},
   "source": [
    "## combine stranger and friend data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f400d825",
   "metadata": {},
   "outputs": [],
   "source": [
    "long_gap_connection['condition'] = 'strangers'\n",
    "long_gap_connection_friends['condition'] = 'friends'\n",
    "\n",
    "long_gaps_all = pd.merge(long_gap_connection, long_gap_connection_friends, how='outer')\n",
    "\n",
    "long_gaps_all.to_csv(os.path.join(base_dir, 'Analyses',\n",
    "                              'long_gap_connection_all.csv'),\n",
    "                        encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265fbd40",
   "metadata": {},
   "source": [
    "# correcting for multiple comparisions\n",
    "\n",
    "Sometime in between first running these analyses and returning to check them before submitting the paper, the statsmodels multitest function stopped working (https://www.statsmodels.org/dev/generated/statsmodels.stats.multitest.multipletests.html). I therefore copy and pasted the source code for the function below. Source code came from here: https://www.statsmodels.org/dev/_modules/statsmodels/stats/multitest.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e1630260",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _ecdf(x):\n",
    "    '''no frills empirical cdf used in fdrcorrection\n",
    "    '''\n",
    "    nobs = len(x)\n",
    "    return np.arange(1,nobs+1)/float(nobs)\n",
    "\n",
    "multitest_methods_names = {'b': 'Bonferroni',\n",
    "                           's': 'Sidak',\n",
    "                           'h': 'Holm',\n",
    "                           'hs': 'Holm-Sidak',\n",
    "                           'sh': 'Simes-Hochberg',\n",
    "                           'ho': 'Hommel',\n",
    "                           'fdr_bh': 'FDR Benjamini-Hochberg',\n",
    "                           'fdr_by': 'FDR Benjamini-Yekutieli',\n",
    "                           'fdr_tsbh': 'FDR 2-stage Benjamini-Hochberg',\n",
    "                           'fdr_tsbky': 'FDR 2-stage Benjamini-Krieger-Yekutieli',\n",
    "                           'fdr_gbs': 'FDR adaptive Gavrilov-Benjamini-Sarkar'\n",
    "                           }\n",
    "\n",
    "_alias_list = [['b', 'bonf', 'bonferroni'],\n",
    "               ['s', 'sidak'],\n",
    "               ['h', 'holm'],\n",
    "               ['hs', 'holm-sidak'],\n",
    "               ['sh', 'simes-hochberg'],\n",
    "               ['ho', 'hommel'],\n",
    "               ['fdr_bh', 'fdr_i', 'fdr_p', 'fdri', 'fdrp'],\n",
    "               ['fdr_by', 'fdr_n', 'fdr_c', 'fdrn', 'fdrcorr'],\n",
    "               ['fdr_tsbh', 'fdr_2sbh'],\n",
    "               ['fdr_tsbky', 'fdr_2sbky', 'fdr_twostage'],\n",
    "               ['fdr_gbs']\n",
    "               ]\n",
    "\n",
    "\n",
    "multitest_alias = {}\n",
    "for m in _alias_list:\n",
    "    multitest_alias[m[0]] = m[0]\n",
    "    for a in m[1:]:\n",
    "        multitest_alias[a] = m[0]\n",
    "\n",
    "def multipletests(pvals, alpha=0.05, method='hs', is_sorted=False,\n",
    "                  returnsorted=False):\n",
    "    \"\"\"\n",
    "    Test results and p-value correction for multiple tests\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    pvals : array_like, 1-d\n",
    "        uncorrected p-values.   Must be 1-dimensional.\n",
    "    alpha : float\n",
    "        FWER, family-wise error rate, e.g. 0.1\n",
    "    method : str\n",
    "        Method used for testing and adjustment of pvalues. Can be either the\n",
    "        full name or initial letters. Available methods are:\n",
    "\n",
    "        - `bonferroni` : one-step correction\n",
    "        - `sidak` : one-step correction\n",
    "        - `holm-sidak` : step down method using Sidak adjustments\n",
    "        - `holm` : step-down method using Bonferroni adjustments\n",
    "        - `simes-hochberg` : step-up method  (independent)\n",
    "        - `hommel` : closed method based on Simes tests (non-negative)\n",
    "        - `fdr_bh` : Benjamini/Hochberg  (non-negative)\n",
    "        - `fdr_by` : Benjamini/Yekutieli (negative)\n",
    "        - `fdr_tsbh` : two stage fdr correction (non-negative)\n",
    "        - `fdr_tsbky` : two stage fdr correction (non-negative)\n",
    "\n",
    "    is_sorted : bool\n",
    "        If False (default), the p_values will be sorted, but the corrected\n",
    "        pvalues are in the original order. If True, then it assumed that the\n",
    "        pvalues are already sorted in ascending order.\n",
    "    returnsorted : bool\n",
    "         not tested, return sorted p-values instead of original sequence\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    reject : ndarray, boolean\n",
    "        true for hypothesis that can be rejected for given alpha\n",
    "    pvals_corrected : ndarray\n",
    "        p-values corrected for multiple tests\n",
    "    alphacSidak : float\n",
    "        corrected alpha for Sidak method\n",
    "    alphacBonf : float\n",
    "        corrected alpha for Bonferroni method\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    There may be API changes for this function in the future.\n",
    "\n",
    "    Except for 'fdr_twostage', the p-value correction is independent of the\n",
    "    alpha specified as argument. In these cases the corrected p-values\n",
    "    can also be compared with a different alpha. In the case of 'fdr_twostage',\n",
    "    the corrected p-values are specific to the given alpha, see\n",
    "    ``fdrcorrection_twostage``.\n",
    "\n",
    "    The 'fdr_gbs' procedure is not verified against another package, p-values\n",
    "    are derived from scratch and are not derived in the reference. In Monte\n",
    "    Carlo experiments the method worked correctly and maintained the false\n",
    "    discovery rate.\n",
    "\n",
    "    All procedures that are included, control FWER or FDR in the independent\n",
    "    case, and most are robust in the positively correlated case.\n",
    "\n",
    "    `fdr_gbs`: high power, fdr control for independent case and only small\n",
    "    violation in positively correlated case\n",
    "\n",
    "    **Timing**:\n",
    "\n",
    "    Most of the time with large arrays is spent in `argsort`. When\n",
    "    we want to calculate the p-value for several methods, then it is more\n",
    "    efficient to presort the pvalues, and put the results back into the\n",
    "    original order outside of the function.\n",
    "\n",
    "    Method='hommel' is very slow for large arrays, since it requires the\n",
    "    evaluation of n partitions, where n is the number of p-values.\n",
    "    \"\"\"\n",
    "    import gc\n",
    "    pvals = np.asarray(pvals)\n",
    "    alphaf = alpha  # Notation ?\n",
    "\n",
    "    if not is_sorted:\n",
    "        sortind = np.argsort(pvals)\n",
    "        pvals = np.take(pvals, sortind)\n",
    "\n",
    "    ntests = len(pvals)\n",
    "    alphacSidak = 1 - np.power((1. - alphaf), 1./ntests)\n",
    "    alphacBonf = alphaf / float(ntests)\n",
    "    if method.lower() in ['b', 'bonf', 'bonferroni']:\n",
    "        reject = pvals <= alphacBonf\n",
    "        pvals_corrected = pvals * float(ntests)\n",
    "\n",
    "    elif method.lower() in ['s', 'sidak']:\n",
    "        reject = pvals <= alphacSidak\n",
    "        pvals_corrected = -np.expm1(ntests * np.log1p(-pvals))\n",
    "\n",
    "    elif method.lower() in ['hs', 'holm-sidak']:\n",
    "        alphacSidak_all = 1 - np.power((1. - alphaf),\n",
    "                                       1./np.arange(ntests, 0, -1))\n",
    "        notreject = pvals > alphacSidak_all\n",
    "        del alphacSidak_all\n",
    "\n",
    "        nr_index = np.nonzero(notreject)[0]\n",
    "        if nr_index.size == 0:\n",
    "            # nonreject is empty, all rejected\n",
    "            notrejectmin = len(pvals)\n",
    "        else:\n",
    "            notrejectmin = np.min(nr_index)\n",
    "        notreject[notrejectmin:] = True\n",
    "        reject = ~notreject\n",
    "        del notreject\n",
    "\n",
    "        # It's eqivalent to 1 - np.power((1. - pvals),\n",
    "        #                           np.arange(ntests, 0, -1))\n",
    "        # but prevents the issue of the floating point precision\n",
    "        pvals_corrected_raw = -np.expm1(np.arange(ntests, 0, -1) *\n",
    "                                        np.log1p(-pvals))\n",
    "        pvals_corrected = np.maximum.accumulate(pvals_corrected_raw)\n",
    "        del pvals_corrected_raw\n",
    "\n",
    "    elif method.lower() in ['h', 'holm']:\n",
    "        notreject = pvals > alphaf / np.arange(ntests, 0, -1)\n",
    "        nr_index = np.nonzero(notreject)[0]\n",
    "        if nr_index.size == 0:\n",
    "            # nonreject is empty, all rejected\n",
    "            notrejectmin = len(pvals)\n",
    "        else:\n",
    "            notrejectmin = np.min(nr_index)\n",
    "        notreject[notrejectmin:] = True\n",
    "        reject = ~notreject\n",
    "        pvals_corrected_raw = pvals * np.arange(ntests, 0, -1)\n",
    "        pvals_corrected = np.maximum.accumulate(pvals_corrected_raw)\n",
    "        del pvals_corrected_raw\n",
    "        gc.collect()\n",
    "\n",
    "    elif method.lower() in ['sh', 'simes-hochberg']:\n",
    "        alphash = alphaf / np.arange(ntests, 0, -1)\n",
    "        reject = pvals <= alphash\n",
    "        rejind = np.nonzero(reject)\n",
    "        if rejind[0].size > 0:\n",
    "            rejectmax = np.max(np.nonzero(reject))\n",
    "            reject[:rejectmax] = True\n",
    "        pvals_corrected_raw = np.arange(ntests, 0, -1) * pvals\n",
    "        pvals_corrected = np.minimum.accumulate(pvals_corrected_raw[::-1])[::-1]\n",
    "        del pvals_corrected_raw\n",
    "\n",
    "    elif method.lower() in ['ho', 'hommel']:\n",
    "        # we need a copy because we overwrite it in a loop\n",
    "        a = pvals.copy()\n",
    "        for m in range(ntests, 1, -1):\n",
    "            cim = np.min(m * pvals[-m:] / np.arange(1,m+1.))\n",
    "            a[-m:] = np.maximum(a[-m:], cim)\n",
    "            a[:-m] = np.maximum(a[:-m], np.minimum(m * pvals[:-m], cim))\n",
    "        pvals_corrected = a\n",
    "        reject = a <= alphaf\n",
    "\n",
    "    elif method.lower() in ['fdr_bh', 'fdr_i', 'fdr_p', 'fdri', 'fdrp']:\n",
    "        # delegate, call with sorted pvals\n",
    "        reject, pvals_corrected = fdrcorrection(pvals, alpha=alpha,\n",
    "                                                 method='indep',\n",
    "                                                 is_sorted=True)\n",
    "    elif method.lower() in ['fdr_by', 'fdr_n', 'fdr_c', 'fdrn', 'fdrcorr']:\n",
    "        # delegate, call with sorted pvals\n",
    "        reject, pvals_corrected = fdrcorrection(pvals, alpha=alpha,\n",
    "                                                 method='n',\n",
    "                                                 is_sorted=True)\n",
    "    elif method.lower() in ['fdr_tsbky', 'fdr_2sbky', 'fdr_twostage']:\n",
    "        # delegate, call with sorted pvals\n",
    "        reject, pvals_corrected = fdrcorrection_twostage(pvals, alpha=alpha,\n",
    "                                                         method='bky',\n",
    "                                                         is_sorted=True)[:2]\n",
    "    elif method.lower() in ['fdr_tsbh', 'fdr_2sbh']:\n",
    "        # delegate, call with sorted pvals\n",
    "        reject, pvals_corrected = fdrcorrection_twostage(pvals, alpha=alpha,\n",
    "                                                         method='bh',\n",
    "                                                         is_sorted=True)[:2]\n",
    "\n",
    "    elif method.lower() in ['fdr_gbs']:\n",
    "        #adaptive stepdown in Gavrilov, Benjamini, Sarkar, Annals of Statistics 2009\n",
    "##        notreject = pvals > alphaf / np.arange(ntests, 0, -1) #alphacSidak\n",
    "##        notrejectmin = np.min(np.nonzero(notreject))\n",
    "##        notreject[notrejectmin:] = True\n",
    "##        reject = ~notreject\n",
    "\n",
    "        ii = np.arange(1, ntests + 1)\n",
    "        q = (ntests + 1. - ii)/ii * pvals / (1. - pvals)\n",
    "        pvals_corrected_raw = np.maximum.accumulate(q) #up requirementd\n",
    "\n",
    "        pvals_corrected = np.minimum.accumulate(pvals_corrected_raw[::-1])[::-1]\n",
    "        del pvals_corrected_raw\n",
    "        reject = pvals_corrected <= alpha\n",
    "\n",
    "    else:\n",
    "        raise ValueError('method not recognized')\n",
    "\n",
    "    if pvals_corrected is not None: #not necessary anymore\n",
    "        pvals_corrected[pvals_corrected>1] = 1\n",
    "    if is_sorted or returnsorted:\n",
    "        return reject, pvals_corrected, alphacSidak, alphacBonf\n",
    "    else:\n",
    "        pvals_corrected_ = np.empty_like(pvals_corrected)\n",
    "        pvals_corrected_[sortind] = pvals_corrected\n",
    "        del pvals_corrected\n",
    "        reject_ = np.empty_like(reject)\n",
    "        reject_[sortind] = reject\n",
    "        return reject_, pvals_corrected_, alphacSidak, alphacBonf\n",
    "\n",
    "\n",
    "\n",
    "def fdrcorrection(pvals, alpha=0.05, method='indep', is_sorted=False):\n",
    "    '''\n",
    "    pvalue correction for false discovery rate.\n",
    "\n",
    "    This covers Benjamini/Hochberg for independent or positively correlated and\n",
    "    Benjamini/Yekutieli for general or negatively correlated tests.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    pvals : array_like, 1d\n",
    "        Set of p-values of the individual tests.\n",
    "    alpha : float, optional\n",
    "        Family-wise error rate. Defaults to ``0.05``.\n",
    "    method : {'i', 'indep', 'p', 'poscorr', 'n', 'negcorr'}, optional\n",
    "        Which method to use for FDR correction.\n",
    "        ``{'i', 'indep', 'p', 'poscorr'}`` all refer to ``fdr_bh``\n",
    "        (Benjamini/Hochberg for independent or positively\n",
    "        correlated tests). ``{'n', 'negcorr'}`` both refer to ``fdr_by``\n",
    "        (Benjamini/Yekutieli for general or negatively correlated tests).\n",
    "        Defaults to ``'indep'``.\n",
    "    is_sorted : bool, optional\n",
    "        If False (default), the p_values will be sorted, but the corrected\n",
    "        pvalues are in the original order. If True, then it assumed that the\n",
    "        pvalues are already sorted in ascending order.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    rejected : ndarray, bool\n",
    "        True if a hypothesis is rejected, False if not\n",
    "    pvalue-corrected : ndarray\n",
    "        pvalues adjusted for multiple hypothesis testing to limit FDR\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    If there is prior information on the fraction of true hypothesis, then alpha\n",
    "    should be set to ``alpha * m/m_0`` where m is the number of tests,\n",
    "    given by the p-values, and m_0 is an estimate of the true hypothesis.\n",
    "    (see Benjamini, Krieger and Yekuteli)\n",
    "\n",
    "    The two-step method of Benjamini, Krieger and Yekutiel that estimates the number\n",
    "    of false hypotheses will be available (soon).\n",
    "\n",
    "    Both methods exposed via this function (Benjamini/Hochberg, Benjamini/Yekutieli)\n",
    "    are also available in the function ``multipletests``, as ``method=\"fdr_bh\"`` and\n",
    "    ``method=\"fdr_by\"``, respectively.\n",
    "\n",
    "    See also\n",
    "    --------\n",
    "    multipletests\n",
    "\n",
    "    '''\n",
    "    pvals = np.asarray(pvals)\n",
    "    assert pvals.ndim == 1, \"pvals must be 1-dimensional, that is of shape (n,)\"\n",
    "\n",
    "    if not is_sorted:\n",
    "        pvals_sortind = np.argsort(pvals)\n",
    "        pvals_sorted = np.take(pvals, pvals_sortind)\n",
    "    else:\n",
    "        pvals_sorted = pvals  # alias\n",
    "\n",
    "    if method in ['i', 'indep', 'p', 'poscorr']:\n",
    "        ecdffactor = _ecdf(pvals_sorted)\n",
    "    elif method in ['n', 'negcorr']:\n",
    "        cm = np.sum(1./np.arange(1, len(pvals_sorted)+1))   #corrected this\n",
    "        ecdffactor = _ecdf(pvals_sorted) / cm\n",
    "##    elif method in ['n', 'negcorr']:\n",
    "##        cm = np.sum(np.arange(len(pvals)))\n",
    "##        ecdffactor = ecdf(pvals_sorted)/cm\n",
    "    else:\n",
    "        raise ValueError('only indep and negcorr implemented')\n",
    "    reject = pvals_sorted <= ecdffactor*alpha\n",
    "    if reject.any():\n",
    "        rejectmax = max(np.nonzero(reject)[0])\n",
    "        reject[:rejectmax] = True\n",
    "\n",
    "    pvals_corrected_raw = pvals_sorted / ecdffactor\n",
    "    pvals_corrected = np.minimum.accumulate(pvals_corrected_raw[::-1])[::-1]\n",
    "    del pvals_corrected_raw\n",
    "    pvals_corrected[pvals_corrected>1] = 1\n",
    "    if not is_sorted:\n",
    "        pvals_corrected_ = np.empty_like(pvals_corrected)\n",
    "        pvals_corrected_[pvals_sortind] = pvals_corrected\n",
    "        del pvals_corrected\n",
    "        reject_ = np.empty_like(reject)\n",
    "        reject_[pvals_sortind] = reject\n",
    "        return reject_, pvals_corrected_\n",
    "    else:\n",
    "        return reject, pvals_corrected\n",
    "\n",
    "\n",
    "\n",
    "def fdrcorrection_twostage(pvals, alpha=0.05, method='bky', iter=False,\n",
    "                           is_sorted=False):\n",
    "    '''(iterated) two stage linear step-up procedure with estimation of number of true\n",
    "    hypotheses\n",
    "\n",
    "    Benjamini, Krieger and Yekuteli, procedure in Definition 6\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    pvals : array_like\n",
    "        set of p-values of the individual tests.\n",
    "    alpha : float\n",
    "        error rate\n",
    "    method : {'bky', 'bh')\n",
    "        see Notes for details\n",
    "\n",
    "        * 'bky' - implements the procedure in Definition 6 of Benjamini, Krieger\n",
    "           and Yekuteli 2006\n",
    "        * 'bh' - the two stage method of Benjamini and Hochberg\n",
    "\n",
    "    iter : bool\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    rejected : ndarray, bool\n",
    "        True if a hypothesis is rejected, False if not\n",
    "    pvalue-corrected : ndarray\n",
    "        pvalues adjusted for multiple hypotheses testing to limit FDR\n",
    "    m0 : int\n",
    "        ntest - rej, estimated number of true hypotheses\n",
    "    alpha_stages : list of floats\n",
    "        A list of alphas that have been used at each stage\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    The returned corrected p-values are specific to the given alpha, they\n",
    "    cannot be used for a different alpha.\n",
    "\n",
    "    The returned corrected p-values are from the last stage of the fdr_bh\n",
    "    linear step-up procedure (fdrcorrection0 with method='indep') corrected\n",
    "    for the estimated fraction of true hypotheses.\n",
    "    This means that the rejection decision can be obtained with\n",
    "    ``pval_corrected <= alpha``, where ``alpha`` is the original significance\n",
    "    level.\n",
    "    (Note: This has changed from earlier versions (<0.5.0) of statsmodels.)\n",
    "\n",
    "    BKY described several other multi-stage methods, which would be easy to implement.\n",
    "    However, in their simulation the simple two-stage method (with iter=False) was the\n",
    "    most robust to the presence of positive correlation\n",
    "\n",
    "    TODO: What should be returned?\n",
    "\n",
    "    '''\n",
    "    pvals = np.asarray(pvals)\n",
    "\n",
    "    if not is_sorted:\n",
    "        pvals_sortind = np.argsort(pvals)\n",
    "        pvals = np.take(pvals, pvals_sortind)\n",
    "\n",
    "    ntests = len(pvals)\n",
    "    if method == 'bky':\n",
    "        fact = (1.+alpha)\n",
    "        alpha_prime = alpha / fact\n",
    "    elif method == 'bh':\n",
    "        fact = 1.\n",
    "        alpha_prime = alpha\n",
    "    else:\n",
    "        raise ValueError(\"only 'bky' and 'bh' are available as method\")\n",
    "\n",
    "    alpha_stages = [alpha_prime]\n",
    "    rej, pvalscorr = fdrcorrection(pvals, alpha=alpha_prime, method='indep',\n",
    "                                   is_sorted=True)\n",
    "    r1 = rej.sum()\n",
    "    if (r1 == 0) or (r1 == ntests):\n",
    "        return rej, pvalscorr * fact, ntests - r1, alpha_stages\n",
    "    ri_old = r1\n",
    "\n",
    "    while True:\n",
    "        ntests0 = 1.0 * ntests - ri_old\n",
    "        alpha_star = alpha_prime * ntests / ntests0\n",
    "        alpha_stages.append(alpha_star)\n",
    "        #print ntests0, alpha_star\n",
    "        rej, pvalscorr = fdrcorrection(pvals, alpha=alpha_star, method='indep',\n",
    "                                       is_sorted=True)\n",
    "        ri = rej.sum()\n",
    "        if (not iter) or ri == ri_old:\n",
    "            break\n",
    "        elif ri < ri_old:\n",
    "            # prevent cycles and endless loops\n",
    "            raise RuntimeError(\" oops - should not be here\")\n",
    "        ri_old = ri\n",
    "\n",
    "    # make adjustment to pvalscorr to reflect estimated number of Non-Null cases\n",
    "    # decision is then pvalscorr < alpha  (or <=)\n",
    "    pvalscorr *= ntests0 * 1.0 /  ntests\n",
    "    if method == 'bky':\n",
    "        pvalscorr *= (1. + alpha)\n",
    "\n",
    "    if not is_sorted:\n",
    "        pvalscorr_ = np.empty_like(pvalscorr)\n",
    "        pvalscorr_[pvals_sortind] = pvalscorr\n",
    "        del pvalscorr\n",
    "        reject = np.empty_like(rej)\n",
    "        reject[pvals_sortind] = rej\n",
    "        return reject, pvalscorr_, ntests - ri, alpha_stages\n",
    "    else:\n",
    "        return rej, pvalscorr, ntests - ri, alpha_stages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39749b53",
   "metadata": {},
   "source": [
    "## stats -- within convo: strangers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "228e8849",
   "metadata": {},
   "outputs": [],
   "source": [
    "pvals = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "129990bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ttest_1sampResult(statistic=0.7112813876828835, pvalue=0.4771631015885298)\n",
      "df:655\n"
     ]
    }
   ],
   "source": [
    "first = stats.ttest_1samp(long_gap_connection['change_in_connection_3_before'].dropna(), 0) # not sig\n",
    "print(first)\n",
    "print('df:' + str(len(long_gap_connection['change_in_connection_3_before'].dropna())-1))\n",
    "pvals.append(first[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c291ab49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ttest_1sampResult(statistic=1.8164183514423367, pvalue=0.06976319071928287)\n",
      "df:655\n"
     ]
    }
   ],
   "source": [
    "second = stats.ttest_1samp(long_gap_connection['change_in_connection_2_before'].dropna(), 0) # not sig\n",
    "print(second)\n",
    "print('df:' + str(len(long_gap_connection['change_in_connection_2_before'].dropna())-1))\n",
    "pvals.append(second[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3912b7dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ttest_1sampResult(statistic=2.45281395795911, pvalue=0.014434035309558887)\n",
      "df:655\n"
     ]
    }
   ],
   "source": [
    "third = stats.ttest_1samp(long_gap_connection['change_in_connection_1_before'].dropna(), 0)\n",
    "print(third)\n",
    "print('df:' + str(len(long_gap_connection['change_in_connection_1_before'].dropna())-1))\n",
    "pvals.append(third[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0a2e10e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ttest_1sampResult(statistic=-3.422122610641923, pvalue=0.0006600081343947048)\n",
      "df:655\n"
     ]
    }
   ],
   "source": [
    "fourth = stats.ttest_1samp(long_gap_connection['change_in_connection_1_after'].dropna(), 0) \n",
    "print(fourth)\n",
    "print('df:' + str(len(long_gap_connection['change_in_connection_1_after'].dropna())-1))\n",
    "pvals.append(fourth[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f9efe5df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ttest_1sampResult(statistic=-2.1745623441672635, pvalue=0.030020105042069565)\n",
      "df:655\n"
     ]
    }
   ],
   "source": [
    "fifth = stats.ttest_1samp(long_gap_connection['change_in_connection_2_after'].dropna(), 0)\n",
    "print(fifth)\n",
    "print('df:' + str(len(long_gap_connection['change_in_connection_2_after'].dropna())-1))\n",
    "pvals.append(fifth[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "02139e5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ttest_1sampResult(statistic=-1.485605118746222, pvalue=0.13786472575297623)\n",
      "df:655\n"
     ]
    }
   ],
   "source": [
    "sixth = stats.ttest_1samp(long_gap_connection['change_in_connection_3_after'].dropna(), 0)  # not sig\n",
    "print(sixth)\n",
    "print('df:' + str(len(long_gap_connection['change_in_connection_3_after'].dropna())-1))\n",
    "pvals.append(sixth[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "84efcaea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([False, False,  True,  True,  True, False]),\n",
       " array([0.33401417, 0.07325135, 0.03031147, 0.00277203, 0.04202815,\n",
       "        0.11580637]),\n",
       " 0.008512444610847103,\n",
       " 0.008333333333333333)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#statsmodels.stats.multitest.multipletests(pvals, alpha=0.05, method='fdr_tsbky', is_sorted=False, returnsorted=False)\n",
    "# adjust p-values for multiple comparisions\n",
    "\n",
    "multipletests(pvals, alpha=0.05, method='fdr_tsbky', is_sorted=False, returnsorted=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790ef917",
   "metadata": {},
   "source": [
    "## stats -- within convo: friends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "584fe805",
   "metadata": {},
   "outputs": [],
   "source": [
    "pvals = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "162cee5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ttest_1sampResult(statistic=-3.4391851938662317, pvalue=0.000644333906067079)\n",
      "df:403\n"
     ]
    }
   ],
   "source": [
    "first = stats.ttest_1samp(long_gap_connection_friends['change_in_connection_3_before'].dropna(), 0)\n",
    "print(first)\n",
    "print('df:' + str(len(long_gap_connection_friends['change_in_connection_3_before'].dropna())-1))\n",
    "pvals.append(first[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "690b2b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ttest_1sampResult(statistic=-3.390324204538273, pvalue=0.0007670103842778143)\n",
      "df:403\n"
     ]
    }
   ],
   "source": [
    "second = stats.ttest_1samp(long_gap_connection_friends['change_in_connection_2_before'].dropna(), 0)\n",
    "print(second)\n",
    "print('df:' + str(len(long_gap_connection_friends['change_in_connection_2_before'].dropna())-1))\n",
    "pvals.append(second[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b762f504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ttest_1sampResult(statistic=-2.204406394522009, pvalue=0.028060279059486454)\n",
      "df:403\n"
     ]
    }
   ],
   "source": [
    "third = stats.ttest_1samp(long_gap_connection_friends['change_in_connection_1_before'].dropna(), 0)\n",
    "print(third)\n",
    "print('df:' + str(len(long_gap_connection_friends['change_in_connection_1_before'].dropna())-1))\n",
    "pvals.append(third[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "56abf501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ttest_1sampResult(statistic=-1.289175349303128, pvalue=0.19807658995481292)\n",
      "df:403\n"
     ]
    }
   ],
   "source": [
    "fourth = stats.ttest_1samp(long_gap_connection_friends['change_in_connection_1_after'].dropna(), 0) # not sig\n",
    "print(fourth)\n",
    "print('df:' + str(len(long_gap_connection_friends['change_in_connection_1_after'].dropna())-1))\n",
    "pvals.append(fourth[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ddf0bdac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ttest_1sampResult(statistic=-1.541104955960675, pvalue=0.1240760578888544)\n",
      "df:403\n"
     ]
    }
   ],
   "source": [
    "fifth = stats.ttest_1samp(long_gap_connection_friends['change_in_connection_2_after'].dropna(), 0) # not sig\n",
    "print(fifth)\n",
    "print('df:' + str(len(long_gap_connection_friends['change_in_connection_2_after'].dropna())-1))\n",
    "pvals.append(fifth[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ec17a494",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ttest_1sampResult(statistic=-1.726476765154216, pvalue=0.08502803635775963)\n",
      "df:403\n"
     ]
    }
   ],
   "source": [
    "sixth = stats.ttest_1samp(long_gap_connection_friends['change_in_connection_3_after'].dropna(), 0) # not sig\n",
    "print(sixth)\n",
    "print('df:' + str(len(long_gap_connection_friends['change_in_connection_3_after'].dropna())-1))\n",
    "pvals.append(sixth[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4291780b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ True,  True,  True, False, False, False]),\n",
       " array([0.00161072, 0.00161072, 0.03928439, 0.13865361, 0.10422389,\n",
       "        0.08927944]),\n",
       " 0.008512444610847103,\n",
       " 0.008333333333333333)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#statsmodels.stats.multitest.multipletests(pvals, alpha=0.05, method='fdr_tsbky', is_sorted=False, returnsorted=False)\n",
    "# adjust p-values for multiple comparisions\n",
    "\n",
    "multipletests(pvals, alpha=0.05, method='fdr_tsbky', is_sorted=False, returnsorted=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4cfa5e",
   "metadata": {},
   "source": [
    "## stats -- between groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "12be0205",
   "metadata": {},
   "outputs": [],
   "source": [
    "pvals = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e89ff041",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ttest_indResult(statistic=2.8933453972411227, pvalue=0.003892976144933679)\n",
      "df:1058\n"
     ]
    }
   ],
   "source": [
    "first = stats.ttest_ind(long_gap_connection['change_in_connection_3_before'].dropna(), \n",
    "                long_gap_connection_friends['change_in_connection_3_before'].dropna(), \n",
    "                equal_var=False)\n",
    "print(first)\n",
    "print('df:' + str((len(long_gap_connection['change_in_connection_3_before'].dropna()))+len(long_gap_connection_friends['change_in_connection_3_before'].dropna())-2))\n",
    "pvals.append(first[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "05068f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ttest_indResult(statistic=3.718819831849168, pvalue=0.00021174307311687412)\n",
      "df:1058\n"
     ]
    }
   ],
   "source": [
    "second = stats.ttest_ind(long_gap_connection['change_in_connection_2_before'].dropna(), \n",
    "                long_gap_connection_friends['change_in_connection_2_before'].dropna(), \n",
    "                equal_var=False)\n",
    "print(second)\n",
    "print('df:' + str((len(long_gap_connection['change_in_connection_2_before'].dropna()))+len(long_gap_connection_friends['change_in_connection_2_before'].dropna())-2))\n",
    "pvals.append(second[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "36e7ef16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ttest_indResult(statistic=3.284849749162399, pvalue=0.0010572542304095652)\n",
      "df:1058\n"
     ]
    }
   ],
   "source": [
    "third = stats.ttest_ind(long_gap_connection['change_in_connection_1_before'].dropna(), \n",
    "                long_gap_connection_friends['change_in_connection_1_before'].dropna(), \n",
    "                equal_var=False)\n",
    "print(third)\n",
    "print('df:' + str((len(long_gap_connection['change_in_connection_1_before'].dropna()))+len(long_gap_connection_friends['change_in_connection_1_before'].dropna())-2))\n",
    "pvals.append(third[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3f61f58e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ttest_indResult(statistic=-0.23761030902318284, pvalue=0.8122686542506321)\n",
      "df:1058\n"
     ]
    }
   ],
   "source": [
    "fourth = stats.ttest_ind(long_gap_connection['change_in_connection_1_after'].dropna(), \n",
    "                long_gap_connection_friends['change_in_connection_1_after'].dropna(), \n",
    "                equal_var=False)\n",
    "print(fourth)\n",
    "print('df:' + str((len(long_gap_connection['change_in_connection_1_after'].dropna()))+len(long_gap_connection_friends['change_in_connection_1_after'].dropna())-2))\n",
    "pvals.append(fourth[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "04fd934b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ttest_indResult(statistic=0.24934347000150373, pvalue=0.8031715596488865)\n",
      "df:1058\n"
     ]
    }
   ],
   "source": [
    "fifth = stats.ttest_ind(long_gap_connection['change_in_connection_2_after'].dropna(), \n",
    "                long_gap_connection_friends['change_in_connection_2_after'].dropna(), \n",
    "                equal_var=False)\n",
    "print(fifth)\n",
    "print('df:' + str((len(long_gap_connection['change_in_connection_2_after'].dropna()))+len(long_gap_connection_friends['change_in_connection_2_after'].dropna())-2))\n",
    "pvals.append(fifth[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "29fcdd59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ttest_indResult(statistic=0.66735379988914, pvalue=0.5047606588354114)\n",
      "df:1058\n"
     ]
    }
   ],
   "source": [
    "sixth = stats.ttest_ind(long_gap_connection['change_in_connection_3_after'].dropna(), \n",
    "                long_gap_connection_friends['change_in_connection_3_after'].dropna(), \n",
    "                equal_var=False)\n",
    "print(sixth)\n",
    "print('df:' + str((len(long_gap_connection['change_in_connection_3_after'].dropna()))+len(long_gap_connection_friends['change_in_connection_3_after'].dropna())-2))\n",
    "pvals.append(sixth[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f9462699",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ True,  True,  True, False, False, False]),\n",
       " array([0.00408762, 0.00066699, 0.00166518, 0.42644104, 0.42644104,\n",
       "        0.39749902]),\n",
       " 0.008512444610847103,\n",
       " 0.008333333333333333)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#statsmodels.stats.multitest.multipletests(pvals, alpha=0.05, method='fdr_tsbky', is_sorted=False, returnsorted=False)\n",
    "# adjust p-values for multiple comparisions\n",
    "\n",
    "multipletests(pvals, alpha=0.05, method='fdr_tsbky', is_sorted=False, returnsorted=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd16d249",
   "metadata": {},
   "source": [
    "# Output a long-form version for plotting purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4e2b15f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "long_gap_connection = pd.DataFrame()\n",
    "long_gap_connection = long_gap_connection.fillna(0)\n",
    "counter = 0\n",
    "\n",
    "flist = glob.glob(os.path.join(base_dir, 'Analyses', 'turn_taking', 'strangers', '*.csv'))\n",
    "\n",
    "for file in flist:\n",
    "    \n",
    "    id_1 = file.split('/')[-1].split('_')[0]\n",
    "    id_2 = file.split('_')[-1].split('.csv')[0]\n",
    "    \n",
    "    data = pd.read_csv(file)\n",
    "    \n",
    "    long_gaps = data.loc[data['gap_length'] > 2000]\n",
    "    \n",
    "    if len(long_gaps) > 0:\n",
    "        \n",
    "        connection_1 = pd.read_csv(os.path.join(base_dir, 'Data', 'continuous_connection_ratings', 'strangers', '{}_{}.csv'.format(id_1, id_2)))\n",
    "        connection_2 = pd.read_csv(os.path.join(base_dir, 'Data', 'continuous_connection_ratings', 'strangers', '{}_{}.csv'.format(id_2, id_1)))\n",
    "        \n",
    "        connection_1['time_msec'] = connection_1['adjustedTime'] * 1000\n",
    "        connection_2['time_msec'] = connection_2['adjustedTime'] * 1000\n",
    "        \n",
    "        for i in list(long_gaps.index): \n",
    "            \n",
    "            if (i > 0):\n",
    "                \n",
    "                if (data.at[i-1, 'turn_end_msec'] > 6000) & (data.at[i, 'turn_start_msec'] < 594000): # might change and just give NaNs for ones that are too close\n",
    "\n",
    "                    # the long gap\n",
    "                    \n",
    "                    start_of_gap = data.at[i-1, 'turn_end_msec']\n",
    "                    end_of_gap = data.at[i, 'turn_start_msec']\n",
    "                    \n",
    "                    connection_1_long_gap_subset = connection_1.loc[(connection_1['time_msec'] > start_of_gap) & (connection_1['time_msec'] < end_of_gap)].reset_index(drop=True)\n",
    "                    connection_1_during_long_gap = np.mean(connection_1_long_gap_subset['Rating'])\n",
    "                    \n",
    "                    connection_2_long_gap_subset = connection_2.loc[(connection_2['time_msec'] > start_of_gap) & (connection_2['time_msec'] < end_of_gap)].reset_index(drop=True)\n",
    "                    connection_2_during_long_gap = np.mean(connection_2_long_gap_subset['Rating'])\n",
    "                    \n",
    "                    long_gap_connection.at[counter, 'subID'] = id_1\n",
    "                    long_gap_connection.at[counter, 'partnerID'] = id_2\n",
    "                    long_gap_connection.at[counter, 'dyad'] = id_1 + '_' + id_2\n",
    "                    long_gap_connection.at[counter, 'timepoint'] = 'long_gap'\n",
    "                    long_gap_connection.at[counter, 'connection'] = connection_1_during_long_gap\n",
    "                    long_gap_connection.at[counter, 'connection_change'] = connection_1_during_long_gap - connection_1_during_long_gap\n",
    "                    long_gap_connection.at[counter, 'gap_length'] = data.at[i, 'gap_length']\n",
    "\n",
    "                    counter += 1\n",
    "                    \n",
    "                    long_gap_connection.at[counter, 'subID'] = id_2\n",
    "                    long_gap_connection.at[counter, 'partnerID'] = id_1\n",
    "                    long_gap_connection.at[counter, 'dyad'] = id_1 + '_' + id_2\n",
    "                    long_gap_connection.at[counter, 'timepoint'] = 'long_gap'\n",
    "                    long_gap_connection.at[counter, 'connection'] = connection_2_during_long_gap\n",
    "                    long_gap_connection.at[counter, 'connection_change'] = connection_2_during_long_gap - connection_2_during_long_gap\n",
    "                    long_gap_connection.at[counter, 'gap_length'] = data.at[i, 'gap_length']\n",
    "\n",
    "                    counter += 1\n",
    "                    \n",
    "                    # 1 chunk before long gap\n",
    "\n",
    "                    start_connection_before_1 = start_of_gap - 2000\n",
    "                    end_connection_before_1 = start_of_gap\n",
    "                    \n",
    "                    connection_1_before_1_subset = connection_1.loc[(connection_1['time_msec'] > start_connection_before_1) & (connection_1['time_msec'] < end_connection_before_1)].reset_index(drop=True)\n",
    "                    connection_1_before_long_gap_1 = np.mean(connection_1_before_1_subset['Rating'])\n",
    "                    \n",
    "                    connection_2_before_1_subset = connection_2.loc[(connection_2['time_msec'] > start_connection_before_1) & (connection_2['time_msec'] < end_connection_before_1)].reset_index(drop=True)\n",
    "                    connection_2_before_long_gap_1 = np.mean(connection_2_before_1_subset['Rating'])\n",
    "                    \n",
    "                    long_gap_connection.at[counter, 'subID'] = id_1\n",
    "                    long_gap_connection.at[counter, 'partnerID'] = id_2\n",
    "                    long_gap_connection.at[counter, 'dyad'] = id_1 + '_' + id_2\n",
    "                    long_gap_connection.at[counter, 'timepoint'] = '1_before'\n",
    "                    long_gap_connection.at[counter, 'connection'] = connection_1_before_long_gap_1\n",
    "                    long_gap_connection.at[counter, 'connection_change'] = connection_1_before_long_gap_1 - connection_1_during_long_gap\n",
    "                    long_gap_connection.at[counter, 'gap_length'] = data.at[i, 'gap_length']\n",
    "                    \n",
    "                    counter += 1\n",
    "                    \n",
    "                    long_gap_connection.at[counter, 'subID'] = id_2\n",
    "                    long_gap_connection.at[counter, 'partnerID'] = id_1\n",
    "                    long_gap_connection.at[counter, 'dyad'] = id_1 + '_' + id_2\n",
    "                    long_gap_connection.at[counter, 'timepoint'] = '1_before'\n",
    "                    long_gap_connection.at[counter, 'connection'] = connection_2_before_long_gap_1\n",
    "                    long_gap_connection.at[counter, 'connection_change'] = connection_2_before_long_gap_1 - connection_2_during_long_gap\n",
    "                    long_gap_connection.at[counter, 'gap_length'] = data.at[i, 'gap_length']\n",
    "                    \n",
    "                    counter += 1\n",
    "\n",
    "                    # 2 chunks before long gap\n",
    "\n",
    "                    start_connection_before_2 = start_of_gap - 4000 \n",
    "                    end_connection_before_2 = start_of_gap - 2000\n",
    "                    \n",
    "                    connection_1_before_2_subset = connection_1.loc[(connection_1['time_msec'] > start_connection_before_2) & (connection_1['time_msec'] < end_connection_before_2)].reset_index(drop=True)\n",
    "                    connection_1_before_long_gap_2 = np.mean(connection_1_before_2_subset['Rating'])\n",
    "                    \n",
    "                    connection_2_before_2_subset = connection_2.loc[(connection_2['time_msec'] > start_connection_before_2) & (connection_2['time_msec'] < end_connection_before_2)].reset_index(drop=True)\n",
    "                    connection_2_before_long_gap_2 = np.mean(connection_2_before_2_subset['Rating'])\n",
    "                    \n",
    "                    long_gap_connection.at[counter, 'subID'] = id_1\n",
    "                    long_gap_connection.at[counter, 'partnerID'] = id_2\n",
    "                    long_gap_connection.at[counter, 'dyad'] = id_1 + '_' + id_2\n",
    "                    long_gap_connection.at[counter, 'timepoint'] = '2_before'\n",
    "                    long_gap_connection.at[counter, 'connection'] = connection_1_before_long_gap_2\n",
    "                    long_gap_connection.at[counter, 'connection_change'] = connection_1_before_long_gap_2 - connection_1_during_long_gap\n",
    "                    long_gap_connection.at[counter, 'gap_length'] = data.at[i, 'gap_length']\n",
    "                    \n",
    "                    counter += 1\n",
    "                    \n",
    "                    long_gap_connection.at[counter, 'subID'] = id_2\n",
    "                    long_gap_connection.at[counter, 'partnerID'] = id_1\n",
    "                    long_gap_connection.at[counter, 'dyad'] = id_1 + '_' + id_2\n",
    "                    long_gap_connection.at[counter, 'timepoint'] = '2_before'\n",
    "                    long_gap_connection.at[counter, 'connection'] = connection_2_before_long_gap_2\n",
    "                    long_gap_connection.at[counter, 'connection_change'] = connection_2_before_long_gap_2 - connection_2_during_long_gap\n",
    "                    long_gap_connection.at[counter, 'gap_length'] = data.at[i, 'gap_length']\n",
    "                    \n",
    "                    counter += 1\n",
    "\n",
    "                    # 3 chunks before long gap\n",
    "\n",
    "                    start_connection_before_3 = start_of_gap - 6000\n",
    "                    end_connection_before_3 = start_of_gap - 4000\n",
    "\n",
    "                    connection_1_before_3_subset = connection_1.loc[(connection_1['time_msec'] > start_connection_before_3) & (connection_1['time_msec'] < end_connection_before_3)].reset_index(drop=True)\n",
    "                    connection_1_before_long_gap_3 = np.mean(connection_1_before_3_subset['Rating'])\n",
    "                    \n",
    "                    connection_2_before_3_subset = connection_2.loc[(connection_2['time_msec'] > start_connection_before_3) & (connection_2['time_msec'] < end_connection_before_3)].reset_index(drop=True)\n",
    "                    connection_2_before_long_gap_3 = np.mean(connection_2_before_3_subset['Rating'])\n",
    "                    \n",
    "                    long_gap_connection.at[counter, 'subID'] = id_1\n",
    "                    long_gap_connection.at[counter, 'partnerID'] = id_2\n",
    "                    long_gap_connection.at[counter, 'dyad'] = id_1 + '_' + id_2\n",
    "                    long_gap_connection.at[counter, 'timepoint'] = '3_before'\n",
    "                    long_gap_connection.at[counter, 'connection'] = connection_1_before_long_gap_3\n",
    "                    long_gap_connection.at[counter, 'connection_change'] = connection_1_before_long_gap_3 - connection_1_during_long_gap\n",
    "                    long_gap_connection.at[counter, 'gap_length'] = data.at[i, 'gap_length']\n",
    "                    \n",
    "                    counter += 1\n",
    "                    \n",
    "                    long_gap_connection.at[counter, 'subID'] = id_2\n",
    "                    long_gap_connection.at[counter, 'partnerID'] = id_1\n",
    "                    long_gap_connection.at[counter, 'dyad'] = id_1 + '_' + id_2\n",
    "                    long_gap_connection.at[counter, 'timepoint'] = '3_before'\n",
    "                    long_gap_connection.at[counter, 'connection'] = connection_2_before_long_gap_3\n",
    "                    long_gap_connection.at[counter, 'connection_change'] = connection_2_before_long_gap_3 - connection_2_during_long_gap\n",
    "                    long_gap_connection.at[counter, 'gap_length'] = data.at[i, 'gap_length']\n",
    "                    \n",
    "                    counter += 1\n",
    "\n",
    "                    # 1 chunk after long gap \n",
    "\n",
    "                    start_connection_after_1 = end_of_gap\n",
    "                    end_connection_after_1 = end_of_gap + 2000\n",
    "                    \n",
    "                    connection_1_after_1_subset = connection_1.loc[(connection_1['time_msec'] > start_connection_after_1) & (connection_1['time_msec'] < end_connection_after_1)].reset_index(drop=True)\n",
    "                    connection_1_after_long_gap_1 = np.mean(connection_1_after_1_subset['Rating'])\n",
    "                    \n",
    "                    connection_2_after_1_subset = connection_2.loc[(connection_2['time_msec'] > start_connection_after_1) & (connection_2['time_msec'] < end_connection_after_1)].reset_index(drop=True)\n",
    "                    connection_2_after_long_gap_1 = np.mean(connection_2_after_1_subset['Rating'])\n",
    "                    \n",
    "                    long_gap_connection.at[counter, 'subID'] = id_1\n",
    "                    long_gap_connection.at[counter, 'partnerID'] = id_2\n",
    "                    long_gap_connection.at[counter, 'dyad'] = id_1 + '_' + id_2\n",
    "                    long_gap_connection.at[counter, 'timepoint'] = '1_after'\n",
    "                    long_gap_connection.at[counter, 'connection'] = connection_1_after_long_gap_1\n",
    "                    long_gap_connection.at[counter, 'connection_change'] = connection_1_after_long_gap_1 - connection_1_during_long_gap\n",
    "                    long_gap_connection.at[counter, 'gap_length'] = data.at[i, 'gap_length']\n",
    "                    \n",
    "                    counter += 1\n",
    "                    \n",
    "                    long_gap_connection.at[counter, 'subID'] = id_2\n",
    "                    long_gap_connection.at[counter, 'partnerID'] = id_1\n",
    "                    long_gap_connection.at[counter, 'dyad'] = id_1 + '_' + id_2\n",
    "                    long_gap_connection.at[counter, 'timepoint'] = '1_after'\n",
    "                    long_gap_connection.at[counter, 'connection'] = connection_2_after_long_gap_1\n",
    "                    long_gap_connection.at[counter, 'connection_change'] = connection_2_after_long_gap_1 - connection_2_during_long_gap\n",
    "                    long_gap_connection.at[counter, 'gap_length'] = data.at[i, 'gap_length']\n",
    "\n",
    "                    \n",
    "                    counter += 1\n",
    "\n",
    "                    # 2 chunks after long gap \n",
    "\n",
    "                    start_connection_after_2 = end_of_gap + 2000 \n",
    "                    end_connection_after_2 = end_of_gap + 4000\n",
    "                    \n",
    "                    connection_1_after_2_subset = connection_1.loc[(connection_1['time_msec'] > start_connection_after_2) & (connection_1['time_msec'] < end_connection_after_2)].reset_index(drop=True)\n",
    "                    connection_1_after_long_gap_2 = np.mean(connection_1_after_2_subset['Rating'])\n",
    "                    \n",
    "                    connection_2_after_2_subset = connection_2.loc[(connection_2['time_msec'] > start_connection_after_2) & (connection_2['time_msec'] < end_connection_after_2)].reset_index(drop=True)\n",
    "                    connection_2_after_long_gap_2 = np.mean(connection_2_after_2_subset['Rating'])\n",
    "                    \n",
    "                    long_gap_connection.at[counter, 'subID'] = id_1\n",
    "                    long_gap_connection.at[counter, 'partnerID'] = id_2\n",
    "                    long_gap_connection.at[counter, 'dyad'] = id_1 + '_' + id_2\n",
    "                    long_gap_connection.at[counter, 'timepoint'] = '2_after'\n",
    "                    long_gap_connection.at[counter, 'connection'] = connection_1_after_long_gap_2\n",
    "                    long_gap_connection.at[counter, 'connection_change'] = connection_1_after_long_gap_2 - connection_1_during_long_gap\n",
    "                    long_gap_connection.at[counter, 'gap_length'] = data.at[i, 'gap_length']\n",
    "                    \n",
    "                    counter += 1\n",
    "                    \n",
    "                    long_gap_connection.at[counter, 'subID'] = id_2\n",
    "                    long_gap_connection.at[counter, 'partnerID'] = id_1\n",
    "                    long_gap_connection.at[counter, 'dyad'] = id_1 + '_' + id_2\n",
    "                    long_gap_connection.at[counter, 'timepoint'] = '2_after'\n",
    "                    long_gap_connection.at[counter, 'connection'] = connection_2_after_long_gap_2\n",
    "                    long_gap_connection.at[counter, 'connection_change'] = connection_2_after_long_gap_2 - connection_2_during_long_gap\n",
    "                    long_gap_connection.at[counter, 'gap_length'] = data.at[i, 'gap_length']\n",
    "                    \n",
    "                    counter += 1\n",
    "\n",
    "                    # 3 chunks after long gap \n",
    "\n",
    "                    start_connection_after_3 = end_of_gap + 4000\n",
    "                    end_connection_after_3 = end_of_gap + 6000\n",
    "                    \n",
    "                    connection_1_after_3_subset = connection_1.loc[(connection_1['time_msec'] > start_connection_after_3) & (connection_1['time_msec'] < end_connection_after_3)].reset_index(drop=True)\n",
    "                    connection_1_after_long_gap_3 = np.mean(connection_1_after_3_subset['Rating'])\n",
    "                    \n",
    "                    connection_2_after_3_subset = connection_2.loc[(connection_2['time_msec'] > start_connection_after_3) & (connection_2['time_msec'] < end_connection_after_3)].reset_index(drop=True)\n",
    "                    connection_2_after_long_gap_3 = np.mean(connection_2_after_3_subset['Rating'])\n",
    "\n",
    "                    long_gap_connection.at[counter, 'subID'] = id_1\n",
    "                    long_gap_connection.at[counter, 'partnerID'] = id_2\n",
    "                    long_gap_connection.at[counter, 'dyad'] = id_1 + '_' + id_2\n",
    "                    long_gap_connection.at[counter, 'timepoint'] = '3_after'\n",
    "                    long_gap_connection.at[counter, 'connection'] = connection_1_after_long_gap_3\n",
    "                    long_gap_connection.at[counter, 'connection_change'] = connection_1_after_long_gap_3 - connection_1_during_long_gap\n",
    "                    long_gap_connection.at[counter, 'gap_length'] = data.at[i, 'gap_length']\n",
    "                    \n",
    "                    counter += 1\n",
    "                    \n",
    "                    long_gap_connection.at[counter, 'subID'] = id_2\n",
    "                    long_gap_connection.at[counter, 'partnerID'] = id_1\n",
    "                    long_gap_connection.at[counter, 'dyad'] = id_1 + '_' + id_2\n",
    "                    long_gap_connection.at[counter, 'timepoint'] = '3_after'\n",
    "                    long_gap_connection.at[counter, 'connection'] = connection_2_after_long_gap_3\n",
    "                    long_gap_connection.at[counter, 'connection_change'] = connection_2_after_long_gap_3 - connection_2_during_long_gap\n",
    "                    long_gap_connection.at[counter, 'gap_length'] = data.at[i, 'gap_length']\n",
    "                    \n",
    "                    counter += 1\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d14080f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "long_gap_connection_friends = pd.DataFrame()\n",
    "long_gap_connection_friends = long_gap_connection_friends.fillna(0)\n",
    "counter = 0\n",
    "\n",
    "flist = glob.glob(os.path.join(base_dir, 'Analyses', 'turn_taking', 'friends', '*.csv'))\n",
    "\n",
    "for file in flist:\n",
    "    \n",
    "    id_1 = file.split('/')[-1].split('_')[0]\n",
    "    id_2 = file.split('_')[-1].split('.csv')[0]\n",
    "    \n",
    "    data = pd.read_csv(file)\n",
    "    \n",
    "    long_gaps = data.loc[data['gap_length'] > 2000]\n",
    "    \n",
    "    if len(long_gaps) > 0:\n",
    "        \n",
    "        connection_1 = pd.read_csv(os.path.join(base_dir, 'Data', 'continuous_connection_ratings', 'friends', '{}_{}.csv'.format(id_1, id_2)))\n",
    "        connection_2 = pd.read_csv(os.path.join(base_dir, 'Data', 'continuous_connection_ratings', 'friends', '{}_{}.csv'.format(id_2, id_1)))\n",
    "        \n",
    "        connection_1['time_msec'] = connection_1['adjustedTime'] * 1000\n",
    "        connection_2['time_msec'] = connection_2['adjustedTime'] * 1000\n",
    "        \n",
    "        for i in list(long_gaps.index): \n",
    "            \n",
    "            if (i > 0):\n",
    "                \n",
    "                if (data.at[i-1, 'turn_end_msec'] > 6000) & (data.at[i, 'turn_start_msec'] < 594000): # might change and just give NaNs for ones that are too close\n",
    "\n",
    "                    # the long gap\n",
    "                    \n",
    "                    start_of_gap = data.at[i-1, 'turn_end_msec']\n",
    "                    end_of_gap = data.at[i, 'turn_start_msec']\n",
    "                    \n",
    "                    connection_1_long_gap_subset = connection_1.loc[(connection_1['time_msec'] > start_of_gap) & (connection_1['time_msec'] < end_of_gap)].reset_index(drop=True)\n",
    "                    connection_1_during_long_gap = np.mean(connection_1_long_gap_subset['Rating'])\n",
    "                    \n",
    "                    connection_2_long_gap_subset = connection_2.loc[(connection_2['time_msec'] > start_of_gap) & (connection_2['time_msec'] < end_of_gap)].reset_index(drop=True)\n",
    "                    connection_2_during_long_gap = np.mean(connection_2_long_gap_subset['Rating'])\n",
    "                    \n",
    "                    long_gap_connection_friends.at[counter, 'subID'] = id_1\n",
    "                    long_gap_connection_friends.at[counter, 'partnerID'] = id_2\n",
    "                    long_gap_connection_friends.at[counter, 'dyad'] = id_1 + '_' + id_2\n",
    "                    long_gap_connection_friends.at[counter, 'timepoint'] = 'long_gap'\n",
    "                    long_gap_connection_friends.at[counter, 'connection'] = connection_1_during_long_gap\n",
    "                    long_gap_connection_friends.at[counter, 'connection_change'] = connection_1_during_long_gap - connection_1_during_long_gap\n",
    "                    long_gap_connection_friends.at[counter, 'gap_length'] = data.at[i, 'gap_length']\n",
    "                    \n",
    "                    counter += 1\n",
    "                    \n",
    "                    long_gap_connection_friends.at[counter, 'subID'] = id_2\n",
    "                    long_gap_connection_friends.at[counter, 'partnerID'] = id_1\n",
    "                    long_gap_connection_friends.at[counter, 'dyad'] = id_1 + '_' + id_2\n",
    "                    long_gap_connection_friends.at[counter, 'timepoint'] = 'long_gap'\n",
    "                    long_gap_connection_friends.at[counter, 'connection'] = connection_2_during_long_gap\n",
    "                    long_gap_connection_friends.at[counter, 'connection_change'] = connection_2_during_long_gap - connection_2_during_long_gap\n",
    "                    long_gap_connection_friends.at[counter, 'gap_length'] = data.at[i, 'gap_length']\n",
    "                    \n",
    "                    counter += 1\n",
    "\n",
    "                    # 1 chunk before long gap\n",
    "\n",
    "                    start_connection_before_1 = start_of_gap - 2000\n",
    "                    end_connection_before_1 = start_of_gap\n",
    "                    \n",
    "                    connection_1_before_1_subset = connection_1.loc[(connection_1['time_msec'] > start_connection_before_1) & (connection_1['time_msec'] < end_connection_before_1)].reset_index(drop=True)\n",
    "                    connection_1_before_long_gap_1 = np.mean(connection_1_before_1_subset['Rating'])\n",
    "                    \n",
    "                    connection_2_before_1_subset = connection_2.loc[(connection_2['time_msec'] > start_connection_before_1) & (connection_2['time_msec'] < end_connection_before_1)].reset_index(drop=True)\n",
    "                    connection_2_before_long_gap_1 = np.mean(connection_2_before_1_subset['Rating'])\n",
    "                    \n",
    "                    long_gap_connection_friends.at[counter, 'subID'] = id_1\n",
    "                    long_gap_connection_friends.at[counter, 'partnerID'] = id_2\n",
    "                    long_gap_connection_friends.at[counter, 'dyad'] = id_1 + '_' + id_2\n",
    "                    long_gap_connection_friends.at[counter, 'timepoint'] = '1_before'\n",
    "                    long_gap_connection_friends.at[counter, 'connection'] = connection_1_before_long_gap_1\n",
    "                    long_gap_connection_friends.at[counter, 'connection_change'] = connection_1_before_long_gap_1 - connection_1_during_long_gap\n",
    "                    long_gap_connection_friends.at[counter, 'gap_length'] = data.at[i, 'gap_length']\n",
    "\n",
    "                    \n",
    "                    counter += 1\n",
    "                    \n",
    "                    long_gap_connection_friends.at[counter, 'subID'] = id_2\n",
    "                    long_gap_connection_friends.at[counter, 'partnerID'] = id_1\n",
    "                    long_gap_connection_friends.at[counter, 'dyad'] = id_1 + '_' + id_2\n",
    "                    long_gap_connection_friends.at[counter, 'timepoint'] = '1_before'\n",
    "                    long_gap_connection_friends.at[counter, 'connection'] = connection_2_before_long_gap_1\n",
    "                    long_gap_connection_friends.at[counter, 'connection_change'] = connection_2_before_long_gap_1 - connection_2_during_long_gap\n",
    "                    long_gap_connection_friends.at[counter, 'gap_length'] = data.at[i, 'gap_length']\n",
    "\n",
    "                    \n",
    "                    counter += 1\n",
    "\n",
    "                    # 2 chunks before long gap\n",
    "\n",
    "                    start_connection_before_2 = start_of_gap - 4000 \n",
    "                    end_connection_before_2 = start_of_gap - 2000\n",
    "                    \n",
    "                    connection_1_before_2_subset = connection_1.loc[(connection_1['time_msec'] > start_connection_before_2) & (connection_1['time_msec'] < end_connection_before_2)].reset_index(drop=True)\n",
    "                    connection_1_before_long_gap_2 = np.mean(connection_1_before_2_subset['Rating'])\n",
    "                    \n",
    "                    connection_2_before_2_subset = connection_2.loc[(connection_2['time_msec'] > start_connection_before_2) & (connection_2['time_msec'] < end_connection_before_2)].reset_index(drop=True)\n",
    "                    connection_2_before_long_gap_2 = np.mean(connection_2_before_2_subset['Rating'])\n",
    "                    \n",
    "                    long_gap_connection_friends.at[counter, 'subID'] = id_1\n",
    "                    long_gap_connection_friends.at[counter, 'partnerID'] = id_2\n",
    "                    long_gap_connection_friends.at[counter, 'dyad'] = id_1 + '_' + id_2\n",
    "                    long_gap_connection_friends.at[counter, 'timepoint'] = '2_before'\n",
    "                    long_gap_connection_friends.at[counter, 'connection'] = connection_1_before_long_gap_2\n",
    "                    long_gap_connection_friends.at[counter, 'connection_change'] = connection_1_before_long_gap_2 - connection_1_during_long_gap\n",
    "                    long_gap_connection_friends.at[counter, 'gap_length'] = data.at[i, 'gap_length']\n",
    "\n",
    "                    \n",
    "                    counter += 1\n",
    "                    \n",
    "                    long_gap_connection_friends.at[counter, 'subID'] = id_2\n",
    "                    long_gap_connection_friends.at[counter, 'partnerID'] = id_1\n",
    "                    long_gap_connection_friends.at[counter, 'dyad'] = id_1 + '_' + id_2\n",
    "                    long_gap_connection_friends.at[counter, 'timepoint'] = '2_before'\n",
    "                    long_gap_connection_friends.at[counter, 'connection'] = connection_2_before_long_gap_2\n",
    "                    long_gap_connection_friends.at[counter, 'connection_change'] = connection_2_before_long_gap_2 - connection_2_during_long_gap\n",
    "                    long_gap_connection_friends.at[counter, 'gap_length'] = data.at[i, 'gap_length']\n",
    "\n",
    "                    \n",
    "                    counter += 1\n",
    "\n",
    "                    # 3 chunks before long gap\n",
    "\n",
    "                    start_connection_before_3 = start_of_gap - 6000\n",
    "                    end_connection_before_3 = start_of_gap - 4000\n",
    "\n",
    "                    connection_1_before_3_subset = connection_1.loc[(connection_1['time_msec'] > start_connection_before_3) & (connection_1['time_msec'] < end_connection_before_3)].reset_index(drop=True)\n",
    "                    connection_1_before_long_gap_3 = np.mean(connection_1_before_3_subset['Rating'])\n",
    "                    \n",
    "                    connection_2_before_3_subset = connection_2.loc[(connection_2['time_msec'] > start_connection_before_3) & (connection_2['time_msec'] < end_connection_before_3)].reset_index(drop=True)\n",
    "                    connection_2_before_long_gap_3 = np.mean(connection_2_before_3_subset['Rating'])\n",
    "                    \n",
    "                    long_gap_connection_friends.at[counter, 'subID'] = id_1\n",
    "                    long_gap_connection_friends.at[counter, 'partnerID'] = id_2\n",
    "                    long_gap_connection_friends.at[counter, 'dyad'] = id_1 + '_' + id_2\n",
    "                    long_gap_connection_friends.at[counter, 'timepoint'] = '3_before'\n",
    "                    long_gap_connection_friends.at[counter, 'connection'] = connection_1_before_long_gap_3\n",
    "                    long_gap_connection_friends.at[counter, 'connection_change'] = connection_1_before_long_gap_3 - connection_1_during_long_gap\n",
    "                    long_gap_connection_friends.at[counter, 'gap_length'] = data.at[i, 'gap_length']\n",
    "\n",
    "                    \n",
    "                    counter += 1\n",
    "                    \n",
    "                    long_gap_connection_friends.at[counter, 'subID'] = id_2\n",
    "                    long_gap_connection_friends.at[counter, 'partnerID'] = id_1\n",
    "                    long_gap_connection_friends.at[counter, 'dyad'] = id_1 + '_' + id_2\n",
    "                    long_gap_connection_friends.at[counter, 'timepoint'] = '3_before'\n",
    "                    long_gap_connection_friends.at[counter, 'connection'] = connection_2_before_long_gap_3\n",
    "                    long_gap_connection_friends.at[counter, 'connection_change'] = connection_2_before_long_gap_3 - connection_2_during_long_gap\n",
    "                    long_gap_connection_friends.at[counter, 'gap_length'] = data.at[i, 'gap_length']\n",
    "\n",
    "                    \n",
    "                    counter += 1\n",
    "\n",
    "                    # 1 chunk after long gap \n",
    "\n",
    "                    start_connection_after_1 = end_of_gap\n",
    "                    end_connection_after_1 = end_of_gap + 2000\n",
    "                    \n",
    "                    connection_1_after_1_subset = connection_1.loc[(connection_1['time_msec'] > start_connection_after_1) & (connection_1['time_msec'] < end_connection_after_1)].reset_index(drop=True)\n",
    "                    connection_1_after_long_gap_1 = np.mean(connection_1_after_1_subset['Rating'])\n",
    "                    \n",
    "                    connection_2_after_1_subset = connection_2.loc[(connection_2['time_msec'] > start_connection_after_1) & (connection_2['time_msec'] < end_connection_after_1)].reset_index(drop=True)\n",
    "                    connection_2_after_long_gap_1 = np.mean(connection_2_after_1_subset['Rating'])\n",
    "                    \n",
    "                    long_gap_connection_friends.at[counter, 'subID'] = id_1\n",
    "                    long_gap_connection_friends.at[counter, 'partnerID'] = id_2\n",
    "                    long_gap_connection_friends.at[counter, 'dyad'] = id_1 + '_' + id_2\n",
    "                    long_gap_connection_friends.at[counter, 'timepoint'] = '1_after'\n",
    "                    long_gap_connection_friends.at[counter, 'connection'] = connection_1_after_long_gap_1\n",
    "                    long_gap_connection_friends.at[counter, 'connection_change'] = connection_1_after_long_gap_1 - connection_1_during_long_gap\n",
    "                    long_gap_connection_friends.at[counter, 'gap_length'] = data.at[i, 'gap_length']\n",
    "\n",
    "                    \n",
    "                    counter += 1\n",
    "                    \n",
    "                    long_gap_connection_friends.at[counter, 'subID'] = id_2\n",
    "                    long_gap_connection_friends.at[counter, 'partnerID'] = id_1\n",
    "                    long_gap_connection_friends.at[counter, 'dyad'] = id_1 + '_' + id_2\n",
    "                    long_gap_connection_friends.at[counter, 'timepoint'] = '1_after'\n",
    "                    long_gap_connection_friends.at[counter, 'connection'] = connection_2_after_long_gap_1\n",
    "                    long_gap_connection_friends.at[counter, 'connection_change'] = connection_2_after_long_gap_1 - connection_2_during_long_gap\n",
    "                    long_gap_connection_friends.at[counter, 'gap_length'] = data.at[i, 'gap_length']\n",
    "\n",
    "                    \n",
    "                    counter += 1\n",
    "\n",
    "                    # 2 chunks after long gap \n",
    "\n",
    "                    start_connection_after_2 = end_of_gap + 2000 \n",
    "                    end_connection_after_2 = end_of_gap + 4000\n",
    "                    \n",
    "                    connection_1_after_2_subset = connection_1.loc[(connection_1['time_msec'] > start_connection_after_2) & (connection_1['time_msec'] < end_connection_after_2)].reset_index(drop=True)\n",
    "                    connection_1_after_long_gap_2 = np.mean(connection_1_after_2_subset['Rating'])\n",
    "                    \n",
    "                    connection_2_after_2_subset = connection_2.loc[(connection_2['time_msec'] > start_connection_after_2) & (connection_2['time_msec'] < end_connection_after_2)].reset_index(drop=True)\n",
    "                    connection_2_after_long_gap_2 = np.mean(connection_2_after_2_subset['Rating'])\n",
    "                    \n",
    "                    long_gap_connection_friends.at[counter, 'subID'] = id_1\n",
    "                    long_gap_connection_friends.at[counter, 'partnerID'] = id_2\n",
    "                    long_gap_connection_friends.at[counter, 'dyad'] = id_1 + '_' + id_2\n",
    "                    long_gap_connection_friends.at[counter, 'timepoint'] = '2_after'\n",
    "                    long_gap_connection_friends.at[counter, 'connection'] = connection_1_after_long_gap_2\n",
    "                    long_gap_connection_friends.at[counter, 'connection_change'] = connection_1_after_long_gap_2 - connection_1_during_long_gap\n",
    "                    long_gap_connection_friends.at[counter, 'gap_length'] = data.at[i, 'gap_length']\n",
    "\n",
    "                    \n",
    "                    counter += 1\n",
    "                    \n",
    "                    long_gap_connection_friends.at[counter, 'subID'] = id_2\n",
    "                    long_gap_connection_friends.at[counter, 'partnerID'] = id_1\n",
    "                    long_gap_connection_friends.at[counter, 'dyad'] = id_1 + '_' + id_2\n",
    "                    long_gap_connection_friends.at[counter, 'timepoint'] = '2_after'\n",
    "                    long_gap_connection_friends.at[counter, 'connection'] = connection_2_after_long_gap_2\n",
    "                    long_gap_connection_friends.at[counter, 'connection_change'] = connection_2_after_long_gap_2 - connection_2_during_long_gap\n",
    "                    long_gap_connection_friends.at[counter, 'gap_length'] = data.at[i, 'gap_length']\n",
    "\n",
    "                    \n",
    "                    counter += 1\n",
    "\n",
    "                    # 3 chunks after long gap \n",
    "\n",
    "                    start_connection_after_3 = end_of_gap + 4000\n",
    "                    end_connection_after_3 = end_of_gap + 6000\n",
    "                    \n",
    "                    connection_1_after_3_subset = connection_1.loc[(connection_1['time_msec'] > start_connection_after_3) & (connection_1['time_msec'] < end_connection_after_3)].reset_index(drop=True)\n",
    "                    connection_1_after_long_gap_3 = np.mean(connection_1_after_3_subset['Rating'])\n",
    "                    \n",
    "                    connection_2_after_3_subset = connection_2.loc[(connection_2['time_msec'] > start_connection_after_3) & (connection_2['time_msec'] < end_connection_after_3)].reset_index(drop=True)\n",
    "                    connection_2_after_long_gap_3 = np.mean(connection_2_after_3_subset['Rating'])\n",
    "\n",
    "                    long_gap_connection_friends.at[counter, 'subID'] = id_1\n",
    "                    long_gap_connection_friends.at[counter, 'partnerID'] = id_2\n",
    "                    long_gap_connection_friends.at[counter, 'dyad'] = id_1 + '_' + id_2\n",
    "                    long_gap_connection_friends.at[counter, 'timepoint'] = '3_after'\n",
    "                    long_gap_connection_friends.at[counter, 'connection'] = connection_1_after_long_gap_3\n",
    "                    long_gap_connection_friends.at[counter, 'connection_change'] = connection_1_after_long_gap_3 - connection_1_during_long_gap\n",
    "                    long_gap_connection_friends.at[counter, 'gap_length'] = data.at[i, 'gap_length']\n",
    "\n",
    "                    \n",
    "                    counter += 1\n",
    "                    \n",
    "                    long_gap_connection_friends.at[counter, 'subID'] = id_2\n",
    "                    long_gap_connection_friends.at[counter, 'partnerID'] = id_1\n",
    "                    long_gap_connection_friends.at[counter, 'dyad'] = id_1 + '_' + id_2\n",
    "                    long_gap_connection_friends.at[counter, 'timepoint'] = '3_after'\n",
    "                    long_gap_connection_friends.at[counter, 'connection'] = connection_2_after_long_gap_3\n",
    "                    long_gap_connection_friends.at[counter, 'connection_change'] = connection_2_after_long_gap_3 - connection_2_during_long_gap\n",
    "                    long_gap_connection_friends.at[counter, 'gap_length'] = data.at[i, 'gap_length']\n",
    "\n",
    "                    \n",
    "                    counter += 1\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9b67de4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "long_gap_connection['condition'] = 'strangers'\n",
    "long_gap_connection_friends['condition'] = 'friends'\n",
    "\n",
    "long_gaps_all = pd.merge(long_gap_connection, long_gap_connection_friends, how='outer')\n",
    "\n",
    "long_gaps_all.to_csv(os.path.join(base_dir, 'Analyses',\n",
    "                              'long_gap_connection_all_long_format.csv'),\n",
    "                        encoding='utf-8', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3_8",
   "language": "python",
   "name": "python3_8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
